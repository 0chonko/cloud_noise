Lmod has detected the following error: The following module(s) are unknown:
"hpcx/hpcx-ompi"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore_cache load "hpcx/hpcx-ompi"

Also make sure that all modulefiles written in TCL start with the string
#%Module



tput: No value for $TERM and no -T specified
tput: No value for $TERM and no -T specified
tput: No value for $TERM and no -T specified
Writing to ../data/oracle-hpc/2024_03_17_23_35_52
Running netgauge OS noise test for 120 seconds ... [Done][175 seconds]
Running netgauge network noise MPI test for 3600 seconds ... [Done][3608 seconds]
Checking MPI_Wtime resolution ... [Done][3 seconds]
Running netgauge LogGP test ... [Done][18 seconds]
Running netgauge one_one MPI test with striping=1 ... [Done][18 seconds]
Running netgauge one_one MPI test with 2 concurrent connections ... [Done][9 seconds]
Running netgauge one_one MPI test with striping=2 ... [Done][7 seconds]
Running netgauge one_one MPI test with 4 concurrent connections ... [Done][20 seconds]
Running netgauge one_one MPI test with striping=4 ... [Done][7 seconds]
Running netgauge one_one MPI test with 8 concurrent connections ... [Done][8 seconds]
Running netgauge one_one MPI test with striping=8 ... [Done][7 seconds]
Running netgauge one_one MPI test with 16 concurrent connections ... [Done][9 seconds]
Running netgauge one_one MPI test with striping=16 ... [Done][12 seconds]
Running netgauge one_one_mpi_bidirect MPI test with striping=1 ... [Done][7 seconds]
Running netgauge one_one_mpi_bidirect MPI test with 2 concurrent connections ... [Done][11 seconds]
Running netgauge one_one_mpi_bidirect MPI test with striping=2 ... [Done][7 seconds]
Running netgauge one_one_mpi_bidirect MPI test with 4 concurrent connections ... [Done][7 seconds]
Running netgauge one_one_mpi_bidirect MPI test with striping=4 ... [Done][10 seconds]
Running netgauge one_one_mpi_bidirect MPI test with 8 concurrent connections ... [Done][8 seconds]
Running netgauge one_one_mpi_bidirect MPI test with striping=8 ... [Done][10 seconds]
Running netgauge one_one_mpi_bidirect MPI test with 16 concurrent connections ... [Done][13 seconds]
Running netgauge one_one_mpi_bidirect MPI test with striping=16 ... [Done][7 seconds]
Running hoverboard test ... [Done][220 seconds]
[Tests completed][4201 seconds]
Compressing ../data/oracle-hpc/2024_03_17_23_35_52 ...
/gpfs/home1/gsavchenko/bench_2/cloud_noise/data/oracle-hpc /gpfs/home1/gsavchenko/bench_2/cloud_noise/benchmarks
2024_03_17_23_35_52/
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_stripe16.cmdout
2024_03_17_23_35_52/ng_netnoise_mpi_lat.out
2024_03_17_23_35_52/ng_osnoise.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_stripe4.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_stripe2.out
2024_03_17_23_35_52/ng_one_one_mpi_conc4.out
2024_03_17_23_35_52/ng_one_one_mpi_conc16.out
2024_03_17_23_35_52/ng_one_one_mpi_conc8.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_conc2.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_stripe4.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_conc2.cmdout
2024_03_17_23_35_52/ng_osnoise.out
2024_03_17_23_35_52/ng_netnoise_mpi_bw.out
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_conc16.out
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_stripe1.out
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_conc16.cmdout
2024_03_17_23_35_52/ng_netnoise_mpi_lat.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_stripe16.out
2024_03_17_23_35_52/ng_loggp.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_stripe1.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_conc8.out
2024_03_17_23_35_52/ng_one_one_mpi_stripe8.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_conc4.out
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_stripe16.out
2024_03_17_23_35_52/ng_one_one_mpi_stripe16.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_stripe8.out
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_stripe4.out
2024_03_17_23_35_52/ng_one_one_mpi_conc16.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_stripe2.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_stripe2.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_conc2.out
2024_03_17_23_35_52/ng_one_one_mpi_conc2.out
2024_03_17_23_35_52/wtime_res.out
2024_03_17_23_35_52/ng_one_one_mpi_conc4.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_conc8.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_stripe1.out
2024_03_17_23_35_52/ng_one_one_mpi_stripe4.out
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_stripe1.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_stripe8.cmdout
2024_03_17_23_35_52/ng_loggp.out
2024_03_17_23_35_52/hoverboard_reps_1.out
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_conc4.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_stripe8.out
2024_03_17_23_35_52/ng_netnoise_mpi_bw.cmdout
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_stripe2.out
2024_03_17_23_35_52/ng_one_one_mpi_bidirect_mpi_conc8.out
/gpfs/home1/gsavchenko/bench_2/cloud_noise/benchmarks

JOB STATISTICS
==============
Job ID: 5579438
Cluster: snellius
User/Group: gsavchenko/gsavchenko
State: COMPLETED (exit code 0)
Nodes: 2
Cores per node: 192
CPU Utilized: 00:52:28
CPU Efficiency: 0.19% of 18-18:59:12 core-walltime
Job Wall-clock time: 01:10:28
Memory Utilized: 589.97 MB
Memory Efficiency: 0.09% of 672.00 GB
