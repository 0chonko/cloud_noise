Lmod has detected the following error: The following module(s) are unknown:
"hpcx/hpcx-ompi"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore_cache load "hpcx/hpcx-ompi"

Also make sure that all modulefiles written in TCL start with the string
#%Module



tput: No value for $TERM and no -T specified
tput: No value for $TERM and no -T specified
tput: No value for $TERM and no -T specified
Writing to ../data/oracle-hpc/2024_03_17_23_36_33
Running netgauge OS noise test for 120 seconds ... [Done][121 seconds]
Running netgauge network noise MPI test for 3600 seconds ... [Done][3608 seconds]
Checking MPI_Wtime resolution ... [Done][4 seconds]
Running netgauge LogGP test ... [Done][17 seconds]
Running netgauge one_one MPI test with striping=1 ... [Done][8 seconds]
Running netgauge one_one MPI test with 2 concurrent connections ... [Done][9 seconds]
Running netgauge one_one MPI test with striping=2 ... [Done][8 seconds]
Running netgauge one_one MPI test with 4 concurrent connections ... [Done][21 seconds]
Running netgauge one_one MPI test with striping=4 ... [Done][8 seconds]
Running netgauge one_one MPI test with 8 concurrent connections ... [Done][9 seconds]
Running netgauge one_one MPI test with striping=8 ... [Done][8 seconds]
Running netgauge one_one MPI test with 16 concurrent connections ... [Done][8 seconds]
Running netgauge one_one MPI test with striping=16 ... [Done][8 seconds]
Running netgauge one_one_mpi_bidirect MPI test with striping=1 ... [Done][9 seconds]
Running netgauge one_one_mpi_bidirect MPI test with 2 concurrent connections ... [Done][8 seconds]
Running netgauge one_one_mpi_bidirect MPI test with striping=2 ... [Done][10 seconds]
Running netgauge one_one_mpi_bidirect MPI test with 4 concurrent connections ... [Done][9 seconds]
Running netgauge one_one_mpi_bidirect MPI test with striping=4 ... [Done][8 seconds]
Running netgauge one_one_mpi_bidirect MPI test with 8 concurrent connections ... [Done][9 seconds]
Running netgauge one_one_mpi_bidirect MPI test with striping=8 ... [Done][12 seconds]
Running netgauge one_one_mpi_bidirect MPI test with 16 concurrent connections ... [Done][8 seconds]
Running netgauge one_one_mpi_bidirect MPI test with striping=16 ... [Done][8 seconds]
Running hoverboard test ... [Done][220 seconds]
[Tests completed][4138 seconds]
Compressing ../data/oracle-hpc/2024_03_17_23_36_33 ...
/gpfs/home1/gsavchenko/bench_2/cloud_noise/data/oracle-hpc /gpfs/home1/gsavchenko/bench_2/cloud_noise/benchmarks
2024_03_17_23_36_33/
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_stripe16.cmdout
2024_03_17_23_36_33/ng_netnoise_mpi_lat.out
2024_03_17_23_36_33/ng_osnoise.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_stripe4.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_stripe2.out
2024_03_17_23_36_33/ng_one_one_mpi_conc4.out
2024_03_17_23_36_33/ng_one_one_mpi_conc16.out
2024_03_17_23_36_33/ng_one_one_mpi_conc8.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_conc2.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_stripe4.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_conc2.cmdout
2024_03_17_23_36_33/ng_osnoise.out
2024_03_17_23_36_33/ng_netnoise_mpi_bw.out
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_conc16.out
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_stripe1.out
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_conc16.cmdout
2024_03_17_23_36_33/ng_netnoise_mpi_lat.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_stripe16.out
2024_03_17_23_36_33/ng_loggp.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_stripe1.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_conc8.out
2024_03_17_23_36_33/ng_one_one_mpi_stripe8.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_conc4.out
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_stripe16.out
2024_03_17_23_36_33/ng_one_one_mpi_stripe16.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_stripe8.out
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_stripe4.out
2024_03_17_23_36_33/ng_one_one_mpi_conc16.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_stripe2.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_stripe2.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_conc2.out
2024_03_17_23_36_33/ng_one_one_mpi_conc2.out
2024_03_17_23_36_33/wtime_res.out
2024_03_17_23_36_33/ng_one_one_mpi_conc4.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_conc8.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_stripe1.out
2024_03_17_23_36_33/ng_one_one_mpi_stripe4.out
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_stripe1.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_stripe8.cmdout
2024_03_17_23_36_33/ng_loggp.out
2024_03_17_23_36_33/hoverboard_reps_1.out
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_conc4.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_stripe8.out
2024_03_17_23_36_33/ng_netnoise_mpi_bw.cmdout
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_stripe2.out
2024_03_17_23_36_33/ng_one_one_mpi_bidirect_mpi_conc8.out
/gpfs/home1/gsavchenko/bench_2/cloud_noise/benchmarks

JOB STATISTICS
==============
Job ID: 5579441
Cluster: snellius
User/Group: gsavchenko/gsavchenko
State: RUNNING
Nodes: 2
Cores per node: 192
CPU Utilized: 00:20:05
CPU Efficiency: 0.08% of 18-11:50:24 core-walltime
Job Wall-clock time: 01:09:21
Memory Utilized: 153.95 MB
Memory Efficiency: 0.02% of 672.00 GB
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
